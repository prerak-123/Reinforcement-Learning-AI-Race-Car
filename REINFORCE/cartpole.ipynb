{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e5BNNzsrgGx",
        "outputId": "7bbb3596-cdfa-4546-a9db-db27cae17ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working with cpu\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Working with {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xCEfKmJergG3"
      },
      "outputs": [],
      "source": [
        "class FCDAP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dims=(32, 32), init_std=1, activation_fc = nn.ReLU()):\n",
        "        super(FCDAP, self).__init__()\n",
        "        self.activation_fc = activation_fc\n",
        "        self.layers = [(nn.Linear(input_dim, hidden_dims[0]))]\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            self.layers.append(self.activation_fc)\n",
        "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
        "        self.layers.append(self.activation_fc)\n",
        "        self.layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = state\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "\n",
        "        return self.layers(x)\n",
        "\n",
        "    def full_pass(self, state):\n",
        "        logits = self.forward(state)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        logpa = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        is_exploratory = action != np.argmax(logits.detach().cpu().numpy())\n",
        "\n",
        "        return action.item(), is_exploratory.item(), logpa, entropy\n",
        "\n",
        "    def select_action(self, state):\n",
        "        logits = self.forward(state).detach()\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample().detach().numpy()\n",
        "        return action\n",
        "\n",
        "    def select_greedy(self, state):\n",
        "        logits = self.forward(state).detach()\n",
        "        return np.argmax(logits.to('cpu').numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_F3Ick8Fscpt"
      },
      "outputs": [],
      "source": [
        "class REINFORCE:\n",
        "    def __init__(self, env, gamma, hidden_dims, optimizer, lr = 1e-4):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.model = FCDAP(env.observation_space.shape[0], env.action_space.n, hidden_dims).to(device)\n",
        "        env.reset()\n",
        "\n",
        "        self.optimizer = optimizer(self.model.parameters(), lr)\n",
        "\n",
        "    def optimize_model(self):\n",
        "        T = len(self.rewards)\n",
        "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
        "        returns = torch.Tensor([np.sum(discounts[:T-t]*self.rewards[t:]) for t in range(T)]).to(device)\n",
        "        self.rewards = torch.Tensor(self.rewards).to(device)\n",
        "        self.logpas = torch.stack(self.logpas).to(device)\n",
        "        discounts = torch.Tensor(discounts).to(device)\n",
        "\n",
        "        policy_loss = -1*(discounts*returns*self.logpas).mean()\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def interactive_step(self, state):\n",
        "        action, _, logpa, _ = self.model.full_pass(state)\n",
        "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.logpas.append(logpa)\n",
        "        return new_state, terminated or truncated\n",
        "\n",
        "    def train(self, num_episodes, step):\n",
        "        self.reward_means = []\n",
        "        for i in range(1, num_episodes + 1):\n",
        "            self.rewards, self.logpas = [], []\n",
        "            terminated = False\n",
        "            state = self.env.reset()[0]\n",
        "\n",
        "            while(not terminated):\n",
        "                state, terminated = self.interactive_step(state)\n",
        "\n",
        "            self.optimize_model()\n",
        "\n",
        "            self.reward_means.append(self.rewards.sum().cpu())\n",
        "\n",
        "            if(i%step == 0):\n",
        "                plt.plot(range(len(self.reward_means)), self.reward_means)\n",
        "                plt.show()\n",
        "                self.reward_means.clear()\n",
        "                self.evaluate()\n",
        "                \n",
        "                # torch.save(self.model.state_dict(), f'./{i/step}_iteration.pth')\n",
        "\n",
        "    def evaluate(self):\n",
        "        terminated = False\n",
        "        state = self.env.reset()[0]\n",
        "        returns = 0\n",
        "        while(not terminated):\n",
        "            img = self.env.render()\n",
        "            cv2.imshow(\"Environment\", img)\n",
        "            # cv2.waitKey(1)\n",
        "            action = self.model.select_greedy(state)\n",
        "            state, reward, terminate, truncated, _ = self.env.step(action)\n",
        "            terminated = truncated or terminate\n",
        "            returns += reward\n",
        "\n",
        "        cv2.destroyAllWindows()\n",
        "        print(f\"Return Obtained: {returns}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "dEZ6Qc0-rgG6",
        "outputId": "641e09a1-b512-4e88-a1ed-39857d29a2ae"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
        "agent = REINFORCE(env, 1, (5, 10), optim.Adam, 5e-5)\n",
        "agent.train(10000, 500)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
